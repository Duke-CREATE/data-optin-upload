{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qq/17cwmxhd3jv_w07w8pf57y5r0000gn/T/ipykernel_69459/290569960.py:10: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Uncomment if text cleaning is desired\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "import time\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def parse_pdf(file):\n",
    "    \"\"\"This function uses PyPDF2 to read a PDF,\n",
    "\n",
    "    Args:\n",
    "        file (FileStorage): The PDF to be parsed\n",
    "\n",
    "    Returns:\n",
    "        text: String\n",
    "    \"\"\"\n",
    "\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "    text = \"\"\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "    for page_num in range(num_pages):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_splitting(text_to_split, chunk_size=5_000, overlap=0.1):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        text (Str): The string that is going to be splitted\n",
    "        tokens (int, optional): The amount of tokens in each batch. Defaults to 1_000.\n",
    "    \"\"\"\n",
    "    overlap = round(chunk_size * overlap)\n",
    "    text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "    texts = text_splitter.split_text(text_to_split)\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "\n",
    "def pdf_embedding(file, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    This function create the embeddings from Open AI\n",
    "    with dimension 1,536\n",
    "\n",
    "\n",
    "    Args:\n",
    "        file (FileStorage): The pdf file\n",
    "    Returns:\n",
    "        a vector of embeddings from\n",
    "    \"\"\"\n",
    "\n",
    "    text = parse_pdf(file)\n",
    "\n",
    "    splitted_text = text_splitting(text)\n",
    "\n",
    "    embeddings = list(map(lambda x: get_embedding(x, model=model), splitted_text))\n",
    "\n",
    "    df = pd.DataFrame({\"document\": splitted_text, \"embedding\": embeddings})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def embeddings_from_type(doc_type: str, file) -> pd.core.frame.DataFrame:\n",
    "\n",
    "    match doc_type:\n",
    "\n",
    "        case \".pdf\":\n",
    "\n",
    "            return pdf_embedding(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35988\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../sample_docs/cs50_harvard.pdf\"\n",
    "with open(file_path, \"rb\") as file:\n",
    "    # Use parse_pdf() on the file\n",
    "    # text, num_pages = parse_pdf(file)\n",
    "    # docs = text_splitting(text)\n",
    "    pdf_embeddings = pdf_embedding(file)\n",
    "    # print(f\"Type docs: {type(docs)}\")\n",
    "    # print(f\"len docs: {len(docs)}\")\n",
    "    # print(f\"Sample:\\n{docs[0][:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Teaching CS50 with AI\\nLeveraging Generative A...</td>\n",
       "      <td>[0.0056123631075024605, 0.003144606249406934, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>replies to\\nthese threads). The CS50 Duck on E...</td>\n",
       "      <td>[0.003431424032896757, 0.01149859931319952, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  Teaching CS50 with AI\\nLeveraging Generative A...   \n",
       "1  replies to\\nthese threads). The CS50 Duck on E...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.0056123631075024605, 0.003144606249406934, ...  \n",
       "1  [0.003431424032896757, 0.01149859931319952, 0....  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index = pc.Index(\"chatarena\")\n",
    "\n",
    "stats = index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new_vectors = pdf_embeddings.shape[0]\n",
    "index_size = stats[\"total_vector_count\"]\n",
    "ids = [f\"ID{i}\" for i in range(index_size + 1, index_size + n_new_vectors + 1)]\n",
    "pdf_embeddings[\"ID\"] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"file name\": \"file.filename\",\n",
    "    \"file type\": \"file_type\",\n",
    "    \"name\": \"owner_name\",\n",
    "    \"subject\": \"subject\",\n",
    "    \"timestamp\": \"timestamp\",\n",
    "    \"blob url\": \"blob_url\",\n",
    "}\n",
    "\n",
    "vectors = [\n",
    "    {\"id\": row[\"ID\"], \"values\": row[\"embedding\"], \"metadata\": metadata}\n",
    "    for _, row in pdf_embeddings.iterrows()\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
